{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from utils import *  # NeuroGraph\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # choose dataset form: ADNI(BOLD), HCP(CORR), BOLD+CORR\n",
    "    dataset = \"ADNI\"\n",
    "    # data path\n",
    "    dataset_dir = \"../../data/ADNI/\"\n",
    "    # choose from: GCNConv, GINConv, SGConv, GeneralConv, GATConv\n",
    "    model = \"GCNConv\" \n",
    "    num_classes = 2  # ADNI - binary classification\n",
    "    weight_decay = 0.0005\n",
    "    batch_size = 16\n",
    "    hidden_mlp = 64\n",
    "    hidden = 32\n",
    "    num_layers = 3\n",
    "    runs = 1\n",
    "    device = \"cpu\" if model != \"GATConv\" else \"cpu\"\n",
    "    lr = 1e-4\n",
    "    epochs = 200\n",
    "    seed = 42\n",
    "args = Args()\n",
    "fix_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading our Datasets\n",
    "use our HCP correlation matrix dataset, train/test split file, label file.\n",
    "\n",
    "HCP data is downloaded from https://drive.google.com/drive/folders/166wCCtPOEL0O25FxzwB0I8AQA8b6Q9U1?usp=drive_link \n",
    "\n",
    "other files are in the data folder\n",
    "\n",
    "use our ADNI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_adni_data():\n",
    "    fMRI_path = args.dataset_dir + \"fmri_signal.mat\"\n",
    "    ICV_path = args.dataset_dir + \"ICV.mat\"\n",
    "    AGE_path = args.dataset_dir + \"AGE.mat\"\n",
    "    DX_path = args.dataset_dir + \"DX.mat\"\n",
    "    gender_path = args.dataset_dir + \"gender.mat\"\n",
    "    fMRI_data_path = args.dataset_dir + \"fMRIdata_ADNI2_ADNI3.csv\"\n",
    "    # participants_path = r'./data/ADNI/participants.tsv'\n",
    "\n",
    "    # read fMRI_path\n",
    "    fmri_data = scipy.io.loadmat(fMRI_path)['fmri_signal']\n",
    "    fMRI_data = [fmri_data[i][0] for i in range(len(fmri_data))]\n",
    "\n",
    "    # read ICV_path\n",
    "    icv_data = scipy.io.loadmat(ICV_path)['ICV']\n",
    "    ICV_data = pd.DataFrame([icv_data[i][0] for i in range(len(icv_data))])\n",
    "\n",
    "    # read AGE_path\n",
    "    age_data = scipy.io.loadmat(AGE_path)['AGE']\n",
    "    AGE_data = pd.DataFrame([age_data[i][0] for i in range(len(age_data))])\n",
    "\n",
    "    # read gender_path\n",
    "    gender_data = scipy.io.loadmat(gender_path)['gender']\n",
    "    gender_data = pd.DataFrame([gender_data[i][0] for i in range(len(gender_data))])\n",
    "\n",
    "    # read DX_path\n",
    "    dx_data = scipy.io.loadmat(DX_path)['DX']\n",
    "    DX_data = pd.DataFrame([dx_data[i][0] for i in range(len(dx_data))])\n",
    "\n",
    "    # for all above variable, add a df.insert(0, 'Image_ID', range(1, 1 + len(fMRI_data))) to add Image_ID column\n",
    "    for df in [ICV_data, AGE_data, gender_data, DX_data]:\n",
    "        df.insert(0, 'Image_ID', range(1, 1 + len(fMRI_data)))\n",
    "\n",
    "    # give their column names, EstimatedTotalIntraCranialVol, Age, Gender, Diagnosis\n",
    "    ICV_data.columns = ['Image_ID', 'EstimatedTotalIntraCranialVol']\n",
    "    AGE_data.columns = ['Image_ID', 'Age']\n",
    "    gender_data.columns = ['Image_ID', 'Gender']\n",
    "    DX_data.columns = ['Image_ID', 'Diagnosis']\n",
    "    Image_ID = ICV_data['Image_ID']\n",
    "\n",
    "    data_dict = {\n",
    "        'fMRI_data': fMRI_data,\n",
    "        'ICV_data': ICV_data,\n",
    "        'AGE_data': AGE_data,\n",
    "        'gender_data': gender_data,\n",
    "        'DX_data': DX_data\n",
    "    }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label path\n",
    "labels_file = args.dataset_dir + 'y.csv'\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "# for ADNI Dataset\n",
    "if args.dataset == \"ADNI\":\n",
    "    adni_data = read_adni_data()\n",
    "    fMRI_data = adni_data['fMRI_data']\n",
    "    ICV_data = adni_data['ICV_data']\n",
    "    AGE_data = adni_data['AGE_data']\n",
    "    gender_data = adni_data['gender_data']\n",
    "    DX_data = adni_data['DX_data']\n",
    "\n",
    "    # only keep healthy control and AD. namely 2 and 0\n",
    "    labels_df = labels_df[labels_df['Diagnosis'].isin([2, 0])].reset_index(drop=True)\n",
    "    # change all 2 to 1\n",
    "    labels_df['Diagnosis'] = labels_df['Diagnosis'].replace({2: 1})\n",
    "\n",
    "    dataset = []\n",
    "    # traverse the labels_df by i\n",
    "    for i in range(len(labels_df)):\n",
    "        # print('i:', i)\n",
    "        IID = labels_df['IID'][i]\n",
    "        y = labels_df['Diagnosis'][i]\n",
    "        # turn y to <class 'torch.Tensor'>\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        # z-score normalization for each column of each subject\n",
    "        subject_data = fMRI_data[IID]\n",
    "        # fill 0 with 1\n",
    "        subject_data[subject_data == 0] = 1\n",
    "        subject_data = (subject_data - np.mean(subject_data, axis=0)) / np.std(subject_data, axis=0)\n",
    "        # x = torch.tensor(subject_data[:100, :], dtype=torch.float)\n",
    "        \n",
    "        edge_attr = pd.read_csv(args.dataset_dir + 'fmri_edge/' + 'pairwise_PC_tHOFC/tHOFC' + str(IID) + '.csv')\n",
    "        x = torch.tensor(edge_attr.to_numpy(), dtype=torch.float)\n",
    "        np.fill_diagonal(edge_attr.to_numpy(), 0)\n",
    "\n",
    "        # 获取10%最大的元素的阈值\n",
    "        threshold = np.percentile(edge_attr, 90)\n",
    "        \n",
    "        # 只保留大于阈值的元素，其他置为0\n",
    "        edge_attr[edge_attr < threshold] = 0\n",
    "\n",
    "        edge_index = np.vstack(np.nonzero(edge_attr.to_numpy()))\n",
    "\n",
    "        # only keep edge_attr with edge_index's value\n",
    "        # 只保留与非零 edge_index 对应的 edge_attr\n",
    "        filtered_edge_attr = edge_attr.to_numpy()[edge_index[0], edge_index[1]]\n",
    "\n",
    "        # 确保 edge_attr 是一维张量\n",
    "        filtered_edge_attr = torch.tensor(filtered_edge_attr, dtype=torch.float)\n",
    "\n",
    "        data = Data(x=x, edge_index=torch.tensor(edge_index, dtype=torch.long), edge_attr=filtered_edge_attr, y=y)\n",
    "\n",
    "\n",
    "        # choose from the special case\n",
    "        if data.edge_index[0].shape[0] != 1000:\n",
    "            continue\n",
    "\n",
    "        dataset.append(data)\n",
    "\n",
    "    # get train and test data\n",
    "    train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "    \n",
    "# for HCP \n",
    "elif args.dataset == \"HCP\":\n",
    "    # train_ids = pd.read_csv(args.dataset_dir + 'ids_train.csv')['IID'].values\n",
    "    # test_ids = pd.read_csv(args.dataset_dir + 'ids_test.csv')['IID'].values\n",
    "    \n",
    "    # train_data = [data for data in (load_mat_data(iid) for iid in train_ids) if data is not None]\n",
    "    # test_data = [data for data in (load_mat_data(iid) for iid in test_ids) if data is not None]\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # # first get a participants df, based on gender, dx, age, image_id\n",
    "# # Assuming you have the following dataframes: gender_data, DX_data, AGE_data, and ICV_data\n",
    "\n",
    "# # Merge gender_data, DX_data, AGE_data, and ICV_data on 'Image_ID'\n",
    "# participants_df = pd.merge(gender_data, DX_data, on='Image_ID')\n",
    "# participants_df = pd.merge(participants_df, AGE_data, on='Image_ID')\n",
    "\n",
    "# # add ICV is to remove the NaN values in the ICV column\n",
    "# participants_df = pd.merge(participants_df, ICV_data, on='Image_ID')\n",
    "\n",
    "# # Rename the columns\n",
    "# participants_df.columns = ['Image_ID', 'Gender', 'Diagnosis', 'Age', 'EstimatedTotalIntraCranialVol']\n",
    "\n",
    "# participants_df = pd.DataFrame(participants_df)\n",
    "# participants_df = participants_df.dropna()\n",
    "\n",
    "# # filter parcitipants_df, only keep Gender is M or F, and Diagnosis is CN or 'Dementia', and change M to 0, F to 1, CN to 1, 'Dementia' to 0\n",
    "\n",
    "# # Create a copy to avoid modifying the original DataFrame\n",
    "# filtered_participants_df = participants_df[(participants_df['Gender'].isin(['M', 'F'])) & \n",
    "#                                            (participants_df['Diagnosis'].isin(['CN', 'MCI', 'Dementia']))].copy()\n",
    "\n",
    "# # Converting categorical values to numeric codes using .loc\n",
    "# filtered_participants_df.loc[:, 'Gender'] = filtered_participants_df['Gender'].replace({'M': 0, 'F': 1})\n",
    "# filtered_participants_df.loc[:, 'Diagnosis'] = filtered_participants_df['Diagnosis'].replace({'CN': 2, 'MCI': 1,'Dementia': 0})\n",
    "\n",
    "# # delete the EstimatedTotalIntraCranialVol column\n",
    "# filtered_participants_df = filtered_participants_df.drop(columns=['EstimatedTotalIntraCranialVol'])\n",
    "\n",
    "# # save this to a participants.tsv file\n",
    "# filtered_participants_df.to_csv(os.path.join(data_path, 'participants.tsv'), index=False)\n",
    "\n",
    "# filtered_Image_ID = filtered_participants_df['Image_ID']\n",
    "\n",
    "# # get HC, MCI, AD in Image_ID\n",
    "# filtered_Image_ID_HC = filtered_participants_df[filtered_participants_df['Diagnosis'] == 2]['Image_ID']\n",
    "# # filtered_Image_ID_MCI = filtered_participants_df[filtered_participants_df['Diagnosis'] == 1]['Image_ID']\n",
    "# filtered_Image_ID_AD = filtered_participants_df[filtered_participants_df['Diagnosis'] == 0]['Image_ID']\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def split_data(image_ids, train_frac=0.8, random_seed=42):\n",
    "#     \"\"\"\n",
    "#     Split the data into training and testing sets.\n",
    "    \n",
    "#     Parameters:\n",
    "#     image_ids (pd.Series): Series of image IDs to be split.\n",
    "#     train_frac (float): Fraction of data to be used for training.\n",
    "#     random_seed (int): Seed for the random number generator.\n",
    "    \n",
    "#     Returns:\n",
    "#     train_ids (pd.Series): Training set of image IDs.\n",
    "#     test_ids (pd.Series): Test set of image IDs.\n",
    "#     \"\"\"\n",
    "#     train_ids = image_ids.sample(frac=train_frac, random_state=random_seed)\n",
    "#     test_ids = image_ids.drop(train_ids.index)\n",
    "#     return train_ids, test_ids\n",
    "\n",
    "# # Apply the function to each category\n",
    "# training_Image_ID_HC, test_Image_ID_HC = split_data(filtered_Image_ID_HC)\n",
    "# # training_Image_ID_MCI, test_Image_ID_MCI = split_data(filtered_Image_ID_MCI)\n",
    "# training_Image_ID_AD, test_Image_ID_AD = split_data(filtered_Image_ID_AD)\n",
    "\n",
    "\n",
    "# # Combine the training and test sets\n",
    "# # training_Image_ID = pd.concat([training_Image_ID_HC, training_Image_ID_MCI, training_Image_ID_AD])\n",
    "# # test_Image_ID = pd.concat([test_Image_ID_HC, test_Image_ID_MCI, test_Image_ID_AD])\n",
    "\n",
    "# training_Image_ID = pd.concat([training_Image_ID_HC, training_Image_ID_AD])\n",
    "# test_Image_ID = pd.concat([test_Image_ID_HC, test_Image_ID_AD])\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # import pandas as pd\n",
    "\n",
    "# # Load the labels data\n",
    "# labels_file = data_path + 'y.csv'\n",
    "# labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "# # Shuffle the data before splitting\n",
    "# labels_df = labels_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Define the train-test split ratio\n",
    "# train_ratio = 0.8\n",
    "# train_size = int(len(labels_df) * train_ratio)\n",
    "\n",
    "# # Split the data into train and test sets\n",
    "# train_df = labels_df[:train_size]\n",
    "# test_df = labels_df[train_size:]\n",
    "\n",
    "# # Extract the 'IID' values for train and test sets\n",
    "# train_ids = train_df['IID'].values\n",
    "# test_ids = test_df['IID'].values\n",
    "\n",
    "# # Display the first few values of train and test IDs\n",
    "# label_train = train_df['Diagnosis'].values\n",
    "# label_test = test_df['Diagnosis'].values\n",
    "# print('train value counts:', np.unique(label_train, return_counts=True))\n",
    "# print('test value counts:', np.unique(label_test, return_counts=True))\n",
    "\n",
    "# # save them to ids_train.csv and ids_test.csv, columns: 'IID'. only ids\n",
    "# # train_df['IID'].to_csv('./data_ADNI/ids_train.csv', index=False)\n",
    "# # test_df['IID'].to_csv('./data_ADNI/ids_test.csv', index=False)\n",
    "# train_df.to_csv(data_path + '/ids_train.csv', index=False)\n",
    "# test_df.to_csv(data_path + '/ids_test.csv', index=False) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Data paths\n",
    "# labels_file = args.dataset_dir + 'y.csv'\n",
    "\n",
    "# Load labels\n",
    "# labels_df = pd.read_csv(labels_file)\n",
    "\n",
    "# # only keep healthy control and AD. namely 2 and 0\n",
    "# labels_df = labels_df[labels_df['Diagnosis'].isin([2, 0])].reset_index(drop=True)\n",
    "\n",
    "# # change all 2 to 1\n",
    "# labels_df['Diagnosis'] = labels_df['Diagnosis'].replace({2: 1})\n",
    "\n",
    "# dataset = []\n",
    "\n",
    "        \n",
    "# traverse the labels_df by i\n",
    "\n",
    "\n",
    "# for i in range(len(labels_df)):\n",
    "#     # print('i:', i)\n",
    "#     IID = labels_df['IID'][i]\n",
    "#     y = labels_df['Diagnosis'][i]\n",
    "#     # turn y to <class 'torch.Tensor'>\n",
    "#     y = torch.tensor(y, dtype=torch.long)\n",
    "#     # z-score normalization for each column of each subject\n",
    "#     subject_data = fMRI_data[i]\n",
    "#     # fill 0 with 1\n",
    "#     subject_data[subject_data == 0] = 1\n",
    "#     subject_data = (subject_data - np.mean(subject_data, axis=0)) / np.std(subject_data, axis=0)\n",
    "#     x = torch.tensor(subject_data[:100, :], dtype=torch.float)\n",
    "#     edge_attr = pd.read_csv(args.dataset_dir + 'fmri_edge/' + 'pearsonpearson_' + str(i + 1) + '.csv')\n",
    "\n",
    "#     np.fill_diagonal(edge_attr.to_numpy(), 0)\n",
    "\n",
    "\n",
    "#     # 获取10%最大的元素的阈值\n",
    "#     threshold = np.percentile(edge_attr, 90)\n",
    "    \n",
    "#     # 只保留大于阈值的元素，其他置为0\n",
    "#     edge_attr[edge_attr < threshold] = 0\n",
    "\n",
    "\n",
    "#     edge_index = np.vstack(np.nonzero(edge_attr.to_numpy()))\n",
    "\n",
    "#     data = Data(x=x, edge_index=torch.tensor(edge_index, dtype=torch.long), edge_attr=torch.tensor(np.nonzero(edge_attr.to_numpy()), dtype=torch.float), y=y)\n",
    "\n",
    "#     if data.edge_index[0].shape[0] != 1000:\n",
    "#         continue    \n",
    "\n",
    "#     dataset.append(data) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test data\n",
    "# train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.125, random_state=123)\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_data, args.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, args.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, args.batch_size, shuffle=False)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1000)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "print(edge_index.shape)  # 输出应该是 [2, num_edges]\n",
    "print(edge_attr.shape)   # 输出应该是 [num_edges]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualGNNs(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(100, 32)\n",
      "    (1-2): 2 x GCNConv(32, 32)\n",
      "  )\n",
      "  (aggr): MeanAggregation()\n",
      "  (bn): BatchNorm1d(5050, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bnh): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=5146, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.038832, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.4964346349745331, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5550387596899224, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5840607210626185, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 1, loss: 0.036735, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.6172156196943974, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.6186046511627907, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6072106261859583, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 2, loss: 0.036811, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.6536842105263158, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5813953488372092, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5601518026565465, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 3, loss: 0.037946, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.7067572156196944, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.6108527131782946, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6091081593927894, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 4, loss: 0.034881, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.7343633276740238, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5705426356589147, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5772296015180266, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 5, loss: 0.036373, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.7418675721561969, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5333333333333333, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5787476280834914, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 6, loss: 0.035032, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.7522580645161291, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5906976744186047, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5586337760910817, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 7, loss: 0.034679, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.7876061120543294, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5705426356589147, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5893738140417457, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 8, loss: 0.03482, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.7976910016977928, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.49457364341085264, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5981024667931689, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 9, loss: 0.034456, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.8180984719864177, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4868217054263565, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6159392789373814, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 10, loss: 0.035061, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.8380984719864176, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.45736434108527135, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5635673624288424, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 11, loss: 0.03501, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.8663157894736842, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.46046511627906983, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5582542694497155, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 12, loss: 0.033591, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.8743972835314092, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.49922480620155035, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5571157495256167, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 13, loss: 0.036412, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.913955857385399, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4883720930232558, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5601518026565465, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 14, loss: 0.034483, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9185059422750423, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4728682170542635, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5681214421252373, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 15, loss: 0.034651, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9070967741935483, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4775193798449612, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5802656546489563, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 16, loss: 0.033195, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9133106960950763, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5054263565891473, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5984819734345351, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 17, loss: 0.032599, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.92, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5193798449612403, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5927893738140417, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 18, loss: 0.032095, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9199320882852292, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4263565891472868, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6246679316888045, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 19, loss: 0.032355, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9227843803056027, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4418604651162791, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.613662239089184, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 20, loss: 0.032753, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9306281833616299, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4744186046511628, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5886148007590133, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 21, loss: 0.03333, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9418675721561969, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4263565891472868, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.603415559772296, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 22, loss: 0.032232, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9506960950764006, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4604651162790697, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6094876660341555, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 23, loss: 0.031961, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9503565365025467, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4294573643410853, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6102466793168881, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 24, loss: 0.033501, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9552801358234294, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.44496124031007755, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6349146110056926, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 25, loss: 0.03199, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9597623089983023, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.40930232558139534, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6235294117647059, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 26, loss: 0.033519, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9594567062818337, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.3829457364341085, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5988614800759013, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 27, loss: 0.033415, \n",
      "train_metrics:{'accuracy': 0.7679012345679013, 'auroc': 0.9565025466893039, 'sensitivity': 1.0, 'specificity': 0.010526315789473684, 'f1_score': 0.8683473389355743}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.43410852713178294, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6174573055028463, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 28, loss: 0.032815, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9624448217317487, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4108527131782945, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6178368121442125, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 29, loss: 0.029496, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9666893039049236, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.3891472868217054, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6110056925996206, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 30, loss: 0.033237, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9799660441426146, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.3798449612403101, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6087286527514232, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 31, loss: 0.030653, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9802037351443122, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.40930232558139534, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6079696394686906, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 32, loss: 0.031079, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9838030560271648, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.448062015503876, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5886148007590134, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 33, loss: 0.030489, \n",
      "train_metrics:{'accuracy': 0.7654320987654321, 'auroc': 0.9859422750424448, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8671328671328671}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4465116279069768, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5977229601518026, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 34, loss: 0.031016, \n",
      "train_metrics:{'accuracy': 0.7728395061728395, 'auroc': 0.984516129032258, 'sensitivity': 1.0, 'specificity': 0.031578947368421054, 'f1_score': 0.8707865168539326}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4170542635658915, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6102466793168881, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 35, loss: 0.03037, \n",
      "train_metrics:{'accuracy': 0.7679012345679013, 'auroc': 0.9846859083191851, 'sensitivity': 1.0, 'specificity': 0.010526315789473684, 'f1_score': 0.8683473389355743}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.45581395348837206, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5946869070208729, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 36, loss: 0.03025, \n",
      "train_metrics:{'accuracy': 0.7753086419753087, 'auroc': 0.9862818336162988, 'sensitivity': 1.0, 'specificity': 0.042105263157894736, 'f1_score': 0.8720112517580872}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.44031007751937984, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6003795066413663, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 37, loss: 0.029438, \n",
      "train_metrics:{'accuracy': 0.7753086419753087, 'auroc': 0.9868251273344653, 'sensitivity': 1.0, 'specificity': 0.042105263157894736, 'f1_score': 0.8720112517580872}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4170542635658914, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5935483870967742, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 38, loss: 0.032574, \n",
      "train_metrics:{'accuracy': 0.7777777777777778, 'auroc': 0.9878777589134126, 'sensitivity': 1.0, 'specificity': 0.05263157894736842, 'f1_score': 0.8732394366197183}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5984819734345351, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 39, loss: 0.03117, \n",
      "train_metrics:{'accuracy': 0.7753086419753087, 'auroc': 0.9906960950764007, 'sensitivity': 1.0, 'specificity': 0.042105263157894736, 'f1_score': 0.8720112517580872}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.38139534883720927, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6022770398481974, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 40, loss: 0.029885, \n",
      "train_metrics:{'accuracy': 0.7851851851851852, 'auroc': 0.9920543293718166, 'sensitivity': 1.0, 'specificity': 0.08421052631578947, 'f1_score': 0.8769448373408769}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.40620155038759687, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5927893738140416, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 41, loss: 0.029203, \n",
      "train_metrics:{'accuracy': 0.7703703703703704, 'auroc': 0.9926655348047537, 'sensitivity': 1.0, 'specificity': 0.021052631578947368, 'f1_score': 0.8695652173913043}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.45271317829457364, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5912713472485768, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 42, loss: 0.028012, \n",
      "train_metrics:{'accuracy': 0.7703703703703704, 'auroc': 0.9952801358234296, 'sensitivity': 1.0, 'specificity': 0.021052631578947368, 'f1_score': 0.8695652173913043}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5100775193798449, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6003795066413662, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 43, loss: 0.029499, \n",
      "train_metrics:{'accuracy': 0.7753086419753087, 'auroc': 0.9962988115449916, 'sensitivity': 1.0, 'specificity': 0.042105263157894736, 'f1_score': 0.8720112517580872}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.5007751937984496, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5935483870967743, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 44, loss: 0.027583, \n",
      "train_metrics:{'accuracy': 0.782716049382716, 'auroc': 0.9958913412563667, 'sensitivity': 1.0, 'specificity': 0.07368421052631578, 'f1_score': 0.8757062146892656}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.48062015503875966, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7241379310344828, 'auroc': 0.5946869070208729, 'sensitivity': 0.9882352941176471, 'specificity': 0.0, 'f1_score': 0.84}\n",
      "epoch: 45, loss: 0.029292, \n",
      "train_metrics:{'accuracy': 0.782716049382716, 'auroc': 0.996332767402377, 'sensitivity': 1.0, 'specificity': 0.07368421052631578, 'f1_score': 0.8757062146892656}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.47906976744186053, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5927893738140417, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 46, loss: 0.027403, \n",
      "train_metrics:{'accuracy': 0.7876543209876543, 'auroc': 0.9955857385398981, 'sensitivity': 1.0, 'specificity': 0.09473684210526316, 'f1_score': 0.8781869688385269}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4589147286821706, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.5836812144212524, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 47, loss: 0.027095, \n",
      "train_metrics:{'accuracy': 0.8123456790123457, 'auroc': 0.9977589134125637, 'sensitivity': 1.0, 'specificity': 0.2, 'f1_score': 0.8908045977011494}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.44341085271317826, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.595066413662239, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 48, loss: 0.02676, \n",
      "train_metrics:{'accuracy': 0.8395061728395061, 'auroc': 0.9980645161290322, 'sensitivity': 1.0, 'specificity': 0.3157894736842105, 'f1_score': 0.9051094890510949}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.43255813953488376, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7241379310344828, 'auroc': 0.5992409867172676, 'sensitivity': 0.9882352941176471, 'specificity': 0.0, 'f1_score': 0.84}\n",
      "epoch: 49, loss: 0.026915, \n",
      "train_metrics:{'accuracy': 0.8098765432098766, 'auroc': 0.9985059422750424, 'sensitivity': 1.0, 'specificity': 0.18947368421052632, 'f1_score': 0.8895265423242468}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4356589147286821, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7241379310344828, 'auroc': 0.6151802656546489, 'sensitivity': 0.9882352941176471, 'specificity': 0.0, 'f1_score': 0.84}\n",
      "epoch: 50, loss: 0.02667, \n",
      "train_metrics:{'accuracy': 0.8296296296296296, 'auroc': 0.997962648556876, 'sensitivity': 1.0, 'specificity': 0.2736842105263158, 'f1_score': 0.8998548621190131}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4325581395348837, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6140417457305503, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 51, loss: 0.026002, \n",
      "train_metrics:{'accuracy': 0.8074074074074075, 'auroc': 0.9995246179966044, 'sensitivity': 1.0, 'specificity': 0.17894736842105263, 'f1_score': 0.8882521489971347}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4635658914728682, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7327586206896551, 'auroc': 0.6227703984819734, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n",
      "epoch: 52, loss: 0.025965, \n",
      "train_metrics:{'accuracy': 0.8271604938271605, 'auroc': 0.9991171477079797, 'sensitivity': 1.0, 'specificity': 0.2631578947368421, 'f1_score': 0.8985507246376812}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4666666666666667, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7241379310344828, 'auroc': 0.6212523719165085, 'sensitivity': 0.9882352941176471, 'specificity': 0.0, 'f1_score': 0.84}\n",
      "epoch: 53, loss: 0.026446, \n",
      "train_metrics:{'accuracy': 0.854320987654321, 'auroc': 0.9977589134125637, 'sensitivity': 1.0, 'specificity': 0.37894736842105264, 'f1_score': 0.9131075110456554}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4852713178294574, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7068965517241379, 'auroc': 0.6208728652751423, 'sensitivity': 0.9647058823529412, 'specificity': 0.0, 'f1_score': 0.8282828282828283}\n",
      "epoch: 54, loss: 0.026889, \n",
      "train_metrics:{'accuracy': 0.837037037037037, 'auroc': 0.9989473684210526, 'sensitivity': 1.0, 'specificity': 0.30526315789473685, 'f1_score': 0.9037900874635568}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.48372093023255813, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7241379310344828, 'auroc': 0.6216318785578747, 'sensitivity': 0.9882352941176471, 'specificity': 0.0, 'f1_score': 0.84}\n",
      "epoch: 55, loss: 0.025832, \n",
      "train_metrics:{'accuracy': 0.8962962962962963, 'auroc': 0.9995246179966044, 'sensitivity': 1.0, 'specificity': 0.5578947368421052, 'f1_score': 0.9365558912386707}, \n",
      "val_metrics:{'accuracy': 0.7241379310344828, 'auroc': 0.4635658914728682, 'sensitivity': 0.9767441860465116, 'specificity': 0.0, 'f1_score': 0.84}, \n",
      "test_metrics:{'accuracy': 0.7068965517241379, 'auroc': 0.6204933586337761, 'sensitivity': 0.9647058823529412, 'specificity': 0.0, 'f1_score': 0.8282828282828283}\n",
      "epoch: 56, loss: 0.024052, \n",
      "train_metrics:{'accuracy': 0.8740740740740741, 'auroc': 1.0, 'sensitivity': 1.0, 'specificity': 0.4631578947368421, 'f1_score': 0.9239940387481371}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4511627906976744, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7155172413793104, 'auroc': 0.6041745730550284, 'sensitivity': 0.9764705882352941, 'specificity': 0.0, 'f1_score': 0.8341708542713567}\n",
      "epoch: 57, loss: 0.024998, \n",
      "train_metrics:{'accuracy': 0.8938271604938272, 'auroc': 1.0, 'sensitivity': 1.0, 'specificity': 0.5473684210526316, 'f1_score': 0.9351432880844646}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.4356589147286821, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.7155172413793104, 'auroc': 0.618595825426945, 'sensitivity': 0.9764705882352941, 'specificity': 0.0, 'f1_score': 0.8341708542713567}\n",
      "epoch: 58, loss: 0.024366, \n",
      "train_metrics:{'accuracy': 0.9259259259259259, 'auroc': 0.9999660441426146, 'sensitivity': 1.0, 'specificity': 0.6842105263157895, 'f1_score': 0.9538461538461539}, \n",
      "val_metrics:{'accuracy': 0.7413793103448276, 'auroc': 0.441860465116279, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.8514851485148515}, \n",
      "test_metrics:{'accuracy': 0.6982758620689655, 'auroc': 0.6041745730550284, 'sensitivity': 0.9529411764705882, 'specificity': 0.0, 'f1_score': 0.8223350253807107}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 121\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m    120\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train(train_loader)\n\u001b[0;32m--> 121\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m test(val_loader)\n\u001b[1;32m    123\u001b[0m     test_metrics \u001b[38;5;241m=\u001b[39m test(test_loader)\n",
      "File \u001b[0;32m~/anaconda3/envs/brainnet/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     22\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 23\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Calculate probabilities\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     preds \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/brainnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/brainnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/task-driven-parcellation/baseline/classification/NeuroGraph/utils.py:89\u001b[0m, in \u001b[0;36mResidualGNNs.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     87\u001b[0m xs \u001b[38;5;241m=\u001b[39m [x]\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m---> 89\u001b[0m     xs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtanh()]\n\u001b[1;32m     91\u001b[0m h \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     92\u001b[0m upper_tri_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu_indices(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/brainnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/brainnet/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/brainnet/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/anaconda3/envs/brainnet/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/brainnet/lib/python3.12/site-packages/torch_geometric/utils/loop.py:651\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    647\u001b[0m     is_undirected \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mis_undirected\n\u001b[1;32m    649\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index[:, mask]\n\u001b[0;32m--> 651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[1;32m    652\u001b[0m     edge_index\u001b[38;5;241m.\u001b[39m_is_undirected \u001b[38;5;241m=\u001b[39m is_undirected\n\u001b[1;32m    654\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_index, loop_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/brainnet/lib/python3.12/site-packages/torch/_jit_internal.py:1120\u001b[0m, in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[0;32m-> 1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;124;03m              return unsupported_linear_op(x)\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:  \n",
    "        data = data.to(args.device)\n",
    "        out = model(data) \n",
    "        loss = criterion(out, data.y) \n",
    "        total_loss +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    return total_loss/len(train_loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(args.device)\n",
    "            out = model(data)\n",
    "            probs = F.softmax(out, dim=1)  # Calculate probabilities\n",
    "            preds = out.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy()[:, 1])  # Keep the probabilities of the positive class\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auroc = roc_auc_score(all_labels, all_probs)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'auroc': auroc,\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# test for multiclass\n",
    "def test_multiclass(loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(args.device)\n",
    "            out = model(data)\n",
    "            probs = F.softmax(out, dim=1)  # 计算所有类的概率\n",
    "            preds = out.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # 计算指标\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    # 如果你需要计算AUROC，对于多分类问题，可以使用平均方法\n",
    "    auroc = roc_auc_score(all_labels, all_probs, multi_class='ovr')  # 'ovr'表示一对多（one-vs-rest）策略\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')  # 使用加权平均\n",
    "    # confusion_matrix 需要转换为多分类版本\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # sensitivity 和 specificity 的计算需要根据每个类分别计算\n",
    "    sensitivities = []\n",
    "    specificities = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i, i]\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        tn = cm.sum() - (tp + fn + fp)\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        sensitivities.append(sensitivity)\n",
    "        specificities.append(specificity)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'auroc': auroc,\n",
    "        'sensitivity': np.mean(sensitivities),\n",
    "        'specificity': np.mean(specificities),\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "checkpoints_dir = './checkpoints/'\n",
    "if not os.path.exists(checkpoints_dir):\n",
    "    os.makedirs(checkpoints_dir)\n",
    "\n",
    "val_acc_history, test_acc_history, test_loss_history = [],[],[]\n",
    "seed = 42\n",
    "for index in range(args.runs):\n",
    "    gnn = eval(args.model)\n",
    "    model = ResidualGNNs(args, train_data, args.hidden, args.hidden_mlp, args.num_layers, gnn).to(args.device) ## apply GNN*\n",
    "    print(model)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    loss, test_acc = [], []\n",
    "    best_val_auroc, best_val_loss = 0.0,0.0\n",
    "    for epoch in range(args.epochs):\n",
    "        loss = train(train_loader)\n",
    "        train_metrics = test(train_loader)\n",
    "        val_metrics = test(val_loader)\n",
    "        test_metrics = test(test_loader)\n",
    "        print(\"epoch: {}, loss: {}, \\ntrain_metrics:{}, \\nval_metrics:{}, \\ntest_metrics:{}\".format(epoch, np.round(loss.item(),6), train_metrics, val_metrics, test_metrics))\n",
    "        \n",
    "        \n",
    "        if val_metrics['auroc'] > best_val_auroc:\n",
    "            best_val_auroc = val_metrics['auroc']\n",
    "            torch.save(model.state_dict(), checkpoints_dir+args.dataset+args.model+'task-checkpoint-best-auroc.pkl')\n",
    "\n",
    "    #test the model\n",
    "    model.load_state_dict(torch.load(checkpoints_dir+args.dataset+args.model+'task-checkpoint-best-auroc.pkl'))\n",
    "    model.eval()\n",
    "    test_acc = test(test_loader)['accuracy']\n",
    "    test_loss = train(test_loader).item()\n",
    "    test_acc_history.append(test_acc)\n",
    "    test_loss_history.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_metrics: {'accuracy': 0.7327586206896551, 'auroc': 0.6072106261859583, 'sensitivity': 1.0, 'specificity': 0.0, 'f1_score': 0.845771144278607}\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(checkpoints_dir+args.dataset+args.model+'task-checkpoint-best-auroc.pkl'))\n",
    "model.eval()\n",
    "test_acc = test(test_loader)['accuracy']\n",
    "test_metrics = test(test_loader)\n",
    "test_loss = train(test_loader).item()\n",
    "test_acc_history.append(test_acc)\n",
    "test_loss_history.append(test_loss)\n",
    "print('test_metrics:', test_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NueroGraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
